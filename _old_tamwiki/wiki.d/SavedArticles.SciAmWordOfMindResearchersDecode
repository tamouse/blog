version=pmwiki-2.2.38 ordered=1 urlencoded=1
agent=Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:14.0) Gecko/20100101 Firefox/14.0.1
author=tamara
charset=ISO-8859-1
csum=add subcategory
ctime=1328052572
host=71.63.215.130
name=SavedArticles.SciAmWordOfMindResearchersDecode
rev=6
targets=Category.Science,Category.NeuroScience,SavedArticles.HomePage,SavedArticles.IncludeMe
text=>>comment%3c%3c%0aTitle:Word-of-Mind: Researchers Decode Words from the Brain's Auditory Activity%0a(:title {*$:Title}:)%0aSummary:Interpreting signals from the brain's language-processing center may improve speech-recognition technology or provide a means for the severely disabled to communicate.%0aSource:http://www.scientificamerican.com/article.cfm?id=word-of-mind-researchers-decode%0aCategories: [[!Science]],[[!NeuroScience]]%0aTags: hearing, research, speech-recognition, brain scans%0a(:tags-hide  hearing, research, speech-recognition, brain scans:)%0aPosted: 2012-01-31 17:09%0aParent:SavedArticles(.HomePage)%0aIncludeMe:[[SavedArticles.HomePage]]%0a>>%3c%3c%0a%0a!!Scientific American%0a%0aBy Daisy Yuhas  | Tuesday, January 31, 2012%0a%0a''[+Oh, to be a fly on the auditory cortex!+]''%0a%0aThat, in a manner of speaking, is exactly what a group of researchers working in Berkeley and San Francisco have done. Measurements of electrical signals in the region of the brain that processes speech enabled the group to decode the words a subject was hearing—in essence, a form of neural eavesdropping.%0a%0a%25rframe%25http://www.scientificamerican.com/media/inline/word-of-mind-researchers-decode_1.jpg\\%0a[-Image: Skip the Budgie, courtesy Flickr-]%0a%0aThe goal was far nobler than finding out what your boss really thinks of you or what is going on in the neighboring cubicle. The research sheds light on how the brain sorts out sounds and turns it into language. "The hope," says Brian Pasley, a post-doctoral researcher at the University of California, Berkeley, and lead author on the study, "is that this knowledge can be utilized to help restore communication in the severely disabled." The work could complement other efforts to reconstruct speech using muscle movements in the vocal tract, lips and tongue.%0a%0aThe researchers—who also hailed from the University of Maryland, College Park, Johns Hopkins University and the University of California, San Francisco—published their work today in `PLoS Biology (pdf).%0a%0aDuring the experiment, subjects listened to words on a loudspeaker or piped through earbuds: sometimes just isolated words like "jazz" or "property"; pseudo-words like "fook" and "nim"; and in a few cases full sentences. Later, the research team studied a record of this activity as it appeared in the brain's auditory cortex, the region that processes what is heard, allowing the comprehension of language, along with other sounds.%0a%0aThe subjects, 15 volunteers with normal language skills, also happened to be undergoing neurosurgical treatments for epilepsy or brain tumors. Because their brain activity was already being monitored at the cortex's surface for seizures, the researchers could examine these direct cortical measurements for their auditory study. Pasley explains that it would have been impossible to have access to such brain scans without these volunteers.%0a%0aPasley and colleagues crafted an algorithm—a computational model—to map the sound a listener was hearing to the electrode's measurements. The model could then "learn" how to match sound to the brain's electrical signals.%0a%0aNext, researchers tested their model by turning the tables: Starting with a listener's brain activity, they used the model to reconstruct the word that a listener had heard. Specifically, the model reconstructed a sound, resembling but not immediately recognizable as a word. To close the loop, the researchers then looked through a set of 47 words to find one that most closely matched the model’s sound.%0a%0aNot only could they successfully "eavesdrop" via cortical activity, the researchers created two versions of their model to account for different features of sound. One version of their computational model made use of a linear representation of sound, called a spectrogram, which plots frequency over time. The other version used a nonlinear representation of sound called a modulation model. Pasley explains that in the linear version, sound rhythms are coded by the brain’s oscillations whereas in the nonlinear version, rhythms are conveyed by the overall level of brain activity. At a slow speech rhythm, both models work well, but at faster rhythms the nonlinear sound representation creates a more accurate model.%0a%0aThis technique could improve speech-recognition technology. Although smart phones do a decent job, anyone who's received a cryptic Google voice transcription knows that speech recognition still is not perfect.%0a%0aThe work parallels research published this fall from U.C. Berkeley in another sensory realm—a computational model that reconstructed the images that subjects were watching in movie trailers.%0a%0aAn obvious follow-up question about Pasley and his colleagues' research: Will this make it possible to read words that we silently vocalize to ourselves, for example, "Oh no, not him again." Pasley explains that the research applies to actual sound a listener hears. Whether the same regions of the brain are involved in the words we sub-vocalize remains unclear.%0a%0aThe experiment demonstrates, though, that it does not take a mind reader to listen in on the subtle processing of the brain at work.%0aScientific American is a trademark of Scientific American, Inc., used with permission%0a%0a© 2012 Scientific American, a Division of Nature America, Inc. All Rights Reserved.%0a
time=1345921460
title=Word-of-Mind: Researchers Decode Words from the Brain's Auditory Activity
author:1345921460=tamara
csum:1345921460=add subcategory
diff:1345921460:1341896859:=6c6%0a%3c Categories: [[!Science]],[[!NeuroScience]]%0a---%0a> Categories: [[!Science]]%0a
host:1345921460=71.63.215.130
author:1341896859=tamara
diff:1341896859:1328053163:=1c1%0a%3c >>comment%3c%3c%0a---%0a> (:if false:)%0a10,12c10%0a%3c Parent:SavedArticles(.HomePage)%0a%3c IncludeMe:[[SavedArticles.HomePage]]%0a%3c >>%3c%3c%0a---%0a> (:ifend:)%0a
host:1341896859=71.63.211.0
author:1328053163=tamara
diff:1328053163:1328052936:=
host:1328053163=71.63.211.0
author:1328052936=tamara
diff:1328052936:1328052900:=
host:1328052936=71.63.211.0
author:1328052900=tamara
diff:1328052900:1328052572:=2,5c2%0a%3c Title:Word-of-Mind: Researchers Decode Words from the Brain's Auditory Activity%0a%3c (:title {*$:Title}:)%0a%3c Summary:Interpreting signals from the brain's language-processing center may improve speech-recognition technology or provide a means for the severely disabled to communicate.%0a%3c Source:http://www.scientificamerican.com/article.cfm?id=word-of-mind-researchers-decode%0a---%0a> Summary:http://www.scientificamerican.com/article.cfm?id=word-of-mind-researchers-decode%0a13a11,16%0a> Permanent Address: http://www.scientificamerican.com/article.cfm?id=word-of-mind-researchers-decode%0a> %0a> !Word-of-Mind: Researchers Decode Words from the Brain's Auditory Activity%0a> %0a> Interpreting signals from the brain's language-processing center may improve speech-recognition technology or provide a means for the severely disabled to communicate%0a> %0a15a19,21%0a> %25rframe%25http://www.scientificamerican.com/media/inline/word-of-mind-researchers-decode_1.jpg\\%0a> [-Image: Skip the Budgie, courtesy Flickr-]%0a> %0a19,21d24%0a%3c %0a%3c %25rframe%25http://www.scientificamerican.com/media/inline/word-of-mind-researchers-decode_1.jpg\\%0a%3c [-Image: Skip the Budgie, courtesy Flickr-]%0a
host:1328052900=71.63.211.0
author:1328052572=tamara
csum:1328052572=new article
diff:1328052572:1328052572:=1,49d0%0a%3c (:if false:)%0a%3c Summary:http://www.scientificamerican.com/article.cfm?id=word-of-mind-researchers-decode%0a%3c Categories: [[!Science]]%0a%3c Tags: hearing, research, speech-recognition, brain scans%0a%3c (:tags-hide  hearing, research, speech-recognition, brain scans:)%0a%3c Posted: 2012-01-31 17:09%0a%3c (:ifend:)%0a%3c %0a%3c !!Scientific American%0a%3c %0a%3c Permanent Address: http://www.scientificamerican.com/article.cfm?id=word-of-mind-researchers-decode%0a%3c %0a%3c !Word-of-Mind: Researchers Decode Words from the Brain's Auditory Activity%0a%3c %0a%3c Interpreting signals from the brain's language-processing center may improve speech-recognition technology or provide a means for the severely disabled to communicate%0a%3c %0a%3c By Daisy Yuhas  | Tuesday, January 31, 2012%0a%3c %0a%3c %25rframe%25http://www.scientificamerican.com/media/inline/word-of-mind-researchers-decode_1.jpg\\%0a%3c [-Image: Skip the Budgie, courtesy Flickr-]%0a%3c %0a%3c ''[+Oh, to be a fly on the auditory cortex!+]''%0a%3c %0a%3c That, in a manner of speaking, is exactly what a group of researchers working in Berkeley and San Francisco have done. Measurements of electrical signals in the region of the brain that processes speech enabled the group to decode the words a subject was hearing—in essence, a form of neural eavesdropping.%0a%3c %0a%3c The goal was far nobler than finding out what your boss really thinks of you or what is going on in the neighboring cubicle. The research sheds light on how the brain sorts out sounds and turns it into language. "The hope," says Brian Pasley, a post-doctoral researcher at the University of California, Berkeley, and lead author on the study, "is that this knowledge can be utilized to help restore communication in the severely disabled." The work could complement other efforts to reconstruct speech using muscle movements in the vocal tract, lips and tongue.%0a%3c %0a%3c The researchers—who also hailed from the University of Maryland, College Park, Johns Hopkins University and the University of California, San Francisco—published their work today in `PLoS Biology (pdf).%0a%3c %0a%3c During the experiment, subjects listened to words on a loudspeaker or piped through earbuds: sometimes just isolated words like "jazz" or "property"; pseudo-words like "fook" and "nim"; and in a few cases full sentences. Later, the research team studied a record of this activity as it appeared in the brain's auditory cortex, the region that processes what is heard, allowing the comprehension of language, along with other sounds.%0a%3c %0a%3c The subjects, 15 volunteers with normal language skills, also happened to be undergoing neurosurgical treatments for epilepsy or brain tumors. Because their brain activity was already being monitored at the cortex's surface for seizures, the researchers could examine these direct cortical measurements for their auditory study. Pasley explains that it would have been impossible to have access to such brain scans without these volunteers.%0a%3c %0a%3c Pasley and colleagues crafted an algorithm—a computational model—to map the sound a listener was hearing to the electrode's measurements. The model could then "learn" how to match sound to the brain's electrical signals.%0a%3c %0a%3c Next, researchers tested their model by turning the tables: Starting with a listener's brain activity, they used the model to reconstruct the word that a listener had heard. Specifically, the model reconstructed a sound, resembling but not immediately recognizable as a word. To close the loop, the researchers then looked through a set of 47 words to find one that most closely matched the model’s sound.%0a%3c %0a%3c Not only could they successfully "eavesdrop" via cortical activity, the researchers created two versions of their model to account for different features of sound. One version of their computational model made use of a linear representation of sound, called a spectrogram, which plots frequency over time. The other version used a nonlinear representation of sound called a modulation model. Pasley explains that in the linear version, sound rhythms are coded by the brain’s oscillations whereas in the nonlinear version, rhythms are conveyed by the overall level of brain activity. At a slow speech rhythm, both models work well, but at faster rhythms the nonlinear sound representation creates a more accurate model.%0a%3c %0a%3c This technique could improve speech-recognition technology. Although smart phones do a decent job, anyone who's received a cryptic Google voice transcription knows that speech recognition still is not perfect.%0a%3c %0a%3c The work parallels research published this fall from U.C. Berkeley in another sensory realm—a computational model that reconstructed the images that subjects were watching in movie trailers.%0a%3c %0a%3c An obvious follow-up question about Pasley and his colleagues' research: Will this make it possible to read words that we silently vocalize to ourselves, for example, "Oh no, not him again." Pasley explains that the research applies to actual sound a listener hears. Whether the same regions of the brain are involved in the words we sub-vocalize remains unclear.%0a%3c %0a%3c The experiment demonstrates, though, that it does not take a mind reader to listen in on the subtle processing of the brain at work.%0a%3c Scientific American is a trademark of Scientific American, Inc., used with permission%0a%3c %0a%3c © 2012 Scientific American, a Division of Nature America, Inc. All Rights Reserved.%0a
host:1328052572=71.63.211.0
